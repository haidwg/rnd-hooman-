{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Artificial Intelligence - COMP9414\n",
        "###Tutorial week 8 - Language Processing\n",
        "\n",
        "NLP methods applied to a Sentiment Analysis use-case scenario\n"
      ],
      "metadata": {
        "id": "S2uHiFBUAGlm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Theoretical Background\n",
        "\n",
        "**Sentiment Analysis** is the task of determining the emotional tone behind a certain text. Sentiment can be classified as either *positive*, *negative* or *neutral* depending on the attitude expressed in the text. The following texts provide examples of positive, negative and neutral sentiment:\n",
        "\n",
        "*   \"This is beautiful! You really did a great job\" -- <font color='green'>Positive</font>\n",
        "*   \"Whaat? This is all wrong! I'm not happy with this at all\" -- <font color='red'>Negative</font>\n",
        "*   \"I guess it could be worse\" -- <font color='orange'>Neutral</font>\n",
        "*   \"Oh yeah, great job breaking my only laptop!\" -- <font color='red'>Negative</font>\n",
        "\n",
        "The last example exhibits a peculiar phenomenon in sentiment analysis called **sarcasm detection** (i.e. sentences that *sound* positive but are really just being sarcastic and conveying a negative sentiment).\n",
        "\n",
        "In this tutorial, we will use a supervised machine learning technique to create an automated sentiment classifier for movie reviews. This will also give us a chance to familiarise with popular NLP techniques (such as **Regular Expressions** and **Context-free Grammars** as well as libraries that researchers in the field use to make their job easier (such as **NLTK** and **Scikit-Learn**).\n"
      ],
      "metadata": {
        "id": "-UjhTnI_Cj4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part 1 - Grammars\n",
        "\n",
        "A **formal grammar** is a set of rules that define how to generate and recognise strings that belong to a certain Language *L*. Grammars are designed to formalise the *syntax* of a language (i.e. how to write things that make sense in that language) and give no information on its *semantics* (i.e. what sentences in that language mean).\n",
        "\n",
        "We can formally define a grammar as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "G=(V,Σ,R,S).\n",
        "\\end{equation}\n",
        "\n",
        "Where\n",
        "\n",
        "$V$ is a finite set of _nonterminal symbols_ or _variables_. These are the symbols used in the grammar to denote syntactic categories.\n",
        "\n",
        "$Σ$ is a finite set of _terminal symbols_ which make up the actual content of the generated sentences. This is also known as the *alphabet* of the language generated by $G$.\n",
        "\n",
        "$R$ is a set of relations defined in $V\\times (V\\cup \\Sigma )^{*}$. These are also known as _Rewrite Rules_ and indicate how grammar variables can be converted into other variables or into terminal symbols.\n",
        "\n",
        "$S$ is the *Start Symbol* used to represent the whole sentence.\n",
        "\n",
        "In this section we will explore some basic generation rules for grammars, using them as a tool to generate training data for our Sentiment Classifier"
      ],
      "metadata": {
        "id": "DQMw2ssp2CZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Section 1.a.__ Write a grammar `greetGrammar` that generates the following three strings:\n",
        "[\"hello\", \"hi\", \"good to see you\"]\n"
      ],
      "metadata": {
        "id": "pPbizsDV57Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "greetGrammar = \"\"\"\n",
        "  S -> \"hello\" | \"hi\" | \"good to see you\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Nq92iKxmyt7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Section 1.b.__ Using NLTK, load your grammar and verify that its language corresponds to the given strings"
      ],
      "metadata": {
        "id": "ow6pr4p86jvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.parse.generate import generate\n",
        "\n",
        "grammar = nltk.CFG.fromstring(greetGrammar)\n",
        "print([g for g in generate(grammar)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3k1MzP_6WX2",
        "outputId": "a33c4b8d-f5b7-4e2e-9ed0-e68a93a1ccbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['hello'], ['hi'], ['good to see you']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Section 1.c.__ Modify the greetGrammar to accept a name after the greeting. The new grammar should parse all greet strings followed by either of these names: [\"Alice\", \"Bob\", \"Charlie\"]. Test your grammar with NLTK to verify that it produces the correct language\n",
        "\n",
        "_HINT:_ use a new non-terminal symbol N to indicate the names and combine it with the existing rules"
      ],
      "metadata": {
        "id": "yzCJ2s9w8I7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "greetGrammarWithNames = \"\"\"\n",
        "  S -> \"hello\" N | \"hi\" N | \"good to see you\" N\n",
        "  N -> \"Alice\" | \"Bob\" | \"Charlie\"\n",
        "\"\"\"\n",
        "#Alternatively\n",
        "#greetGrammarWithNames = \"\"\"\n",
        "#  S -> H N\n",
        "#  H -> \"hello\" | \"hi\" | \"good to see you\"\n",
        "#  N -> \"Alice\" | \"Bob\" | \"Charlie\"\n",
        "#\"\"\"\n",
        "grammar = nltk.CFG.fromstring(greetGrammarWithNames)\n",
        "print([g for g in generate(grammar)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e76f854-98b9-4a15-8948-9fec3fdbf3fd",
        "id": "reTWiMLz8gUk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['hello', 'Alice'], ['hello', 'Bob'], ['hello', 'Charlie'], ['hi', 'Alice'], ['hi', 'Bob'], ['hi', 'Charlie'], ['good to see you', 'Alice'], ['good to see you', 'Bob'], ['good to see you', 'Charlie']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Section 1.d.__ We will now use grammars to automatically generate some data for our Sentiment Analysis classifier. Consider the following grammar that generates positive reviews for films:\n",
        "\n",
        "```\n",
        "S -> NP VP | PR VPR\n",
        "NP -> Det N\n",
        "N -> 'director' | 'screenplay' | 'plot' | 'story' | 'scenes' | 'special effects' | 'costumes' | 'actors' | 'dialogues' | 'characters'\n",
        "\n",
        "VP -> Verb Adj\n",
        "\n",
        "VPR -> VerbPR NP\n",
        "\n",
        "Det -> 'the' | 'this' | 'these' | 'those'\n",
        "\n",
        "Verb -> 'is' | 'looks' | 'was' | 'are' | 'look' | 'were'\n",
        "\n",
        "VerbPR -> 'love' | 'loved' | 'enjoy' | 'enjoyed' | 'fell in love with' | 'adore' | 'adored'\n",
        "PR -> 'I'\n",
        "\n",
        "Adj -> 'great' | 'cool' | 'amazing' | 'fantastic' | 'very nice'\n",
        "```\n",
        "\n",
        "Load the grammar in NLTK and store all generated strings in a variable. Then, print 5 randomly generated strings from the grammar\n"
      ],
      "metadata": {
        "id": "2h2HJDVy8yAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "positive_film_grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP | PR VPR\n",
        "NP -> Det N\n",
        "N -> 'director' | 'screenplay' | 'plot' | 'story' | 'scenes' | 'special effects' | 'costumes' | 'actors' | 'dialogues' | 'characters'\n",
        "\n",
        "VP -> Verb Adj\n",
        "\n",
        "VPR -> VerbPR NP\n",
        "\n",
        "Det -> 'the' | 'this' | 'these' | 'those'\n",
        "\n",
        "Verb -> 'is' | 'looks' | 'was' | 'are' | 'look' | 'were'\n",
        "\n",
        "VerbPR -> 'love' | 'loved' | 'enjoy' | 'enjoyed' | 'fell in love with' | 'adore' | 'adored'\n",
        "PR -> 'I'\n",
        "\n",
        "Adj -> 'great' | 'cool' | 'amazing' | 'fantastic' | 'very nice'\n",
        "\"\"\")\n",
        "\n",
        "positive_reviews = list(generate(positive_film_grammar))\n",
        "for i in range(5):\n",
        "  print(random.choice(positive_reviews))"
      ],
      "metadata": {
        "id": "N8uiJt41F9aP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd8e608-6b25-4dc5-80de-71058684e84e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'loved', 'these', 'costumes']\n",
            "['this', 'scenes', 'were', 'great']\n",
            "['these', 'dialogues', 'look', 'great']\n",
            "['these', 'plot', 'was', 'great']\n",
            "['I', 'loved', 'the', 'actors']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Section 1.e.__ You may have noticed that your grammar generates strings that are not consistent with singular/plural and with first/third person verbs. Fix the grammar so that it only generates strings that are grammatically correct.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1GxhxdXGFFgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_film_grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NPS VPS | NPP VPP | PR VPR\n",
        "NPS -> DetS NS\n",
        "NPP -> DetP NP\n",
        "NS -> 'director' | 'screenplay' | 'plot' | 'story' | 'atmosphere'\n",
        "NP -> 'scenes' | 'special effects' | 'costumes' | 'actors' | 'dialogues' | 'characters'\n",
        "\n",
        "VPS -> VerbS Adj\n",
        "VPP -> VerbP Adj\n",
        "\n",
        "VPR -> VerbPR NPS | VerbPR NPP\n",
        "DetS -> 'the' | 'this'\n",
        "DetP -> 'the' | 'these' | 'those'\n",
        "\n",
        "VerbS -> 'is' | 'looks' | 'was'\n",
        "VerbP -> 'are' | 'look' | 'were'\n",
        "\n",
        "VerbPR -> 'love' | 'loved' | 'enjoy' | 'enjoyed' | 'fell in love with' | 'adore' | 'adored'\n",
        "PR -> 'I'\n",
        "\n",
        "Adj -> 'great' | 'cool' | 'amazing' | 'fantastic' | 'very nice'\n",
        "\"\"\")\n",
        "\n",
        "positive_reviews = [' '.join(s) for s in generate(positive_film_grammar)]\n",
        "for i in range(5):\n",
        "  print(random.choice(positive_reviews))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw2CGtzgnqyU",
        "outputId": "bbd55386-54a3-4027-edb3-13dea0b0329a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I adored those dialogues\n",
            "those special effects look fantastic\n",
            "I love those actors\n",
            "those costumes were very nice\n",
            "I love those special effects\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Section 1.f.__ Modify the `positive_film_grammar` to obtain a `negative_film_grammar` that produces negative reviews."
      ],
      "metadata": {
        "id": "hI6P-JwZCSbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_film_grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NPS VPS | NPP VPP | PR VPR\n",
        "NPS -> DetS NS\n",
        "NPP -> DetP NP\n",
        "NS -> 'director' | 'screenplay' | 'plot' | 'story' | 'atmosphere'\n",
        "NP -> 'scenes' | 'special effects' | 'costumes' | 'actors' | 'dialogues' | 'characters'\n",
        "\n",
        "VPS -> VerbS Adj\n",
        "VPP -> VerbP Adj\n",
        "\n",
        "VPR -> VerbPR NPS | VerbPR NPP\n",
        "DetS -> 'the' | 'this'\n",
        "DetP -> 'the' | 'these' | 'those'\n",
        "\n",
        "VerbS -> 'is' | 'looks' | 'was'\n",
        "VerbP -> 'are' | 'look' | 'were'\n",
        "\n",
        "VerbPR -> 'hate' | 'hated' | 'do not like' | 'did not enjoy' | 'got bored with' | 'despise' | 'despised'\n",
        "PR -> 'I'\n",
        "\n",
        "Adj -> 'mediocre' | 'dull' | 'terrible' | 'boring' | 'lame' | 'dumb'\n",
        "\"\"\")\n",
        "\n",
        "negative_reviews = [' '.join(s) for s in generate(negative_film_grammar)]\n",
        "for i in range(5):\n",
        "  print(random.choice(negative_reviews))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a938cdb7-e525-461e-b5b5-5b8acade85ca",
        "id": "zdoMeLjoCdeT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "those actors are lame\n",
            "those actors are dumb\n",
            "the screenplay is boring\n",
            "I despised the scenes\n",
            "the screenplay was mediocre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Section 1.g.__ Generate a training dataset for the Movie Reviews Sentiment Analysis task. Your dataset should have 1,000 positive reviews obtained by sampling three random utterances from the `positive_reviews` language and concatenating them together, and 1,000 negative reviews obtained by applying the same method to the `negative_reviews` language. Each datapoint should be a tuple T= (utterance, label), where label can be either \"neg\" or \"pos\" depending on the sentiment of the generated datapoint.\n"
      ],
      "metadata": {
        "id": "uQi6YyZGAp8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grammar_training_dataset = []\n",
        "for i in range(1000):\n",
        "  positive_utterance = f\"{random.choice(positive_reviews)}. {random.choice(positive_reviews)}. {random.choice(positive_reviews)}\"\n",
        "  negative_utterance = f\"{random.choice(negative_reviews)}. {random.choice(negative_reviews)}. {random.choice(negative_reviews)}\"\n",
        "  grammar_training_dataset.append((positive_utterance, \"pos\"))\n",
        "  grammar_training_dataset.append((negative_utterance, \"neg\"))\n",
        "\n",
        "for i in range(5):\n",
        "  print(random.choice(grammar_training_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGfN3slDA7d4",
        "outputId": "c4bcadd7-57fd-484f-8ee2-8bb8c7c2ca91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I despise these actors. the characters were terrible. those dialogues are terrible', 'neg')\n",
            "('these dialogues were terrible. the atmosphere is mediocre. those actors are boring', 'neg')\n",
            "('this atmosphere looks cool. the dialogues look great. I adore the plot', 'pos')\n",
            "('the atmosphere is fantastic. this atmosphere was great. the special effects look cool', 'pos')\n",
            "('I hated those scenes. the dialogues were terrible. those scenes look dumb', 'neg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part 2 - Data Preparation\n",
        "\n",
        "A crucial component of every Natural Language Processing application is the *Data Preparation Pipeline*, which converts the data into a format that can be parsed by a statistical machine learning algorithm. This pipeline usually combines at least the following essential steps:\n",
        "\n",
        "- **Data cleanup** (i.e. removing noisy elements from a dataset such as special characters, abbreviations, URLs, multiple spaces etc.)\n",
        "- **Word Embedding** (i.e. converting text data into numerical vectors). This can be performed in a number of different ways, but usually involves the creation of a *Vocabulary*, which maps words to unique numerical IDs.\n",
        "- **Data splitting and collation**, which involves dividing a dataset into different subsets for training, testing and validation of hyperparameters (splitting), as well as combining multiple datapoints into a dense batch for training (collation)\n",
        "\n",
        "We will use NLTK's **Movie Review Corpus** as a dataset for our sentiment classifier. This is a well-known corpus for the task which can be downloaded and imported directly from a Python script using the **NLTK** library (note that we won't tokenize reviews at this stage, as tokenization will be handled later when building the Machine Learning model). We will use _Regular Expressions_ for the Data Cleanup step, relying on Python's internal Regular Expression library, ```regex```. For the other steps, we will rely on **Scikit-Learn**, a machine learning library highly specialised for textual data processing\n",
        "\n"
      ],
      "metadata": {
        "id": "Hhm5pLhYovtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1 - Regular Expressions\n",
        "\n",
        "A regular expression (often shortened as regex) is a sequence of characters that specifies a match pattern in text. This sequence is used in combination with a matching algorithm that finds the pattern in a text, possibly replacing it with another piece of text.\n",
        "\n",
        "There are various implementations of regular expressions, including online portals, UNIX shell commands and libraries for basically every imperative programming language. In this tutorial, we will use Python's ```regex``` module and its ```sub``` function, which finds regular expression patterns and replaces every occurrence with a new piece of text."
      ],
      "metadata": {
        "id": "Ybm9MlKtFPBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Section 2.1.a. (familiarising with Regular Expressions)__ Import the ```regex``` library as ```re```, and use its ```re.sub``` function to replace all occurrences of the word ```\"men\"``` with the word ```\"people\"``` in the following portion of text (do not replace the string if it is a substring of another word):\n",
        "\n",
        "```\"We hold these truths to be self-evident, that all men are created equal, that all men are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness and mental stability.\"```"
      ],
      "metadata": {
        "id": "yLysQdq_G4LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "\n",
        "input_text = \"We hold these truths to be self-evident, that all men are created equal, that all men are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness and mental stability.\"\n",
        "\n",
        "re.sub(r\"\\bmen\\b\", \"people\", input_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3_ENMM45G3xe",
        "outputId": "ef4ca1ad-813d-44b1-f94e-cb9261280de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We hold these truths to be self-evident, that all people are created equal, that all people are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness and mental stability.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Section 2.1.b.__ Load the NLTK Movie Reviews corpus. You will have to download the corpus first with the command:\n",
        "\n",
        "`nltk.download(\"movie_reviews\")`\n",
        "\n",
        "and then you will be able to load it by importing movie_reviews from nltk.corpus. Load the corpus utterances and labels in a list of tuples similarly to the dataset created in Section 1.g"
      ],
      "metadata": {
        "id": "_PU5vIljrQ2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')\n",
        "\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "nltk_data = []\n",
        "for file_id in movie_reviews.fileids():\n",
        "  nltk_data.append((movie_reviews.raw(file_id), movie_reviews.categories(file_id)[0]))\n",
        "\n",
        "print(len(nltk_data))"
      ],
      "metadata": {
        "id": "ZdWIWTpYy5eS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e85a0134-9b70-4533-e300-8e5d6527ddaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Section 2.1.c.__ Using regular expressions, write a function called ```cleanup_review``` which performs the following cleanup operations on your data:\n",
        "\n",
        "1. Replace all URLs with URLTOKEN\n",
        "2. Replace all dates with DATETOKEN\n",
        "3. Remove all non-alphanumerical characters (except for the following: ```[\"?\", \"!\", \",\", \".\", \":\", \";\", \"'\", \"\\\"\"]```)\n",
        "4. Collapse multiple spaces into one space\n",
        "\n",
        "Test your function with the following string:\n",
        "\n",
        "```\"Hello!! My name is Stefano, I have been a tutor for COMP~9414 since 01/04/2023.    My personal website is http://stefano.com . (Nice to meet you ^__^)\"```\n",
        "\n",
        "should return\n",
        "\n",
        "```\"Hello!! My name is Stefano, I have been a tutor for COMP9414 since DATETOKEN. My personal website is URLTOKEN . Nice to meet you\"```\n",
        "```"
      ],
      "metadata": {
        "id": "j4mb-NdMFZI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "\n",
        "def cleanup_review(review):\n",
        "\n",
        "  # 1. Replace all URLs with URLTOKEN\n",
        "  review = re.sub(r'http\\S+', 'URLTOKEN', review)\n",
        "\n",
        "  # 2. Replace all dates with DATETOKEN\n",
        "  review = re.sub(r\"\\d{4}-\\d{2}-\\d{2}\", \"DATETOKEN\", review)\n",
        "  review = re.sub(r\"\\d{4}/\\d{2}/\\d{2}\", \"DATETOKEN\", review)\n",
        "\n",
        "  review = re.sub(r\"\\d{2}-\\d{2}-\\d{4}\", \"DATETOKEN\", review)\n",
        "  review = re.sub(r\"\\d{2}/\\d{2}/\\d{4}\", \"DATETOKEN\", review)\n",
        "\n",
        "\n",
        "  # 3. Remove all non-alphanumerical characters\n",
        "  review = re.sub(r'[^a-zA-Z0-9,!.\\';:? ]', '', review)\n",
        "\n",
        "  # 4. Collapse multiple spaces into one space\n",
        "  review = re.sub(r'\\s\\s+', ' ', review)\n",
        "\n",
        "  return review\n",
        "\n",
        "\n",
        "cleanup_review(\"Hello!! My name is Stefano, I have been a tutor for COMP~9414 since 01/04/2023. My personal website is http://stefano.com . (Nice to meet you ^__^)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9453d88a-5f34-4c87-c10a-bc3027bb56ee",
        "id": "4qM0ShhjOrDr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello!! My name is Stefano, I have been a tutor for COMP9414 since DATETOKEN. My personal website is URLTOKEN . Nice to meet you '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2 - Data splitting and dataset creation"
      ],
      "metadata": {
        "id": "pfNyKzCpewCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Section 2.2.a__ Apply the ```cleanup_review``` function to the NLTK dataset. Then, split it into three subsets:\n",
        "\n",
        "- train_nltk_data (which should contain the first 85% of the reviews (1-1700))\n",
        "- test_nltk_data (which should contain reviews 85%~95% (1701-1900))\n",
        "- valid_nltk_data (which should contain the remaining reviews (1901-2000))\n",
        "\n",
        "Remember to shuffle the cleaned data before splitting to ensure an equal distribution of labels across the three sets. You can set a seed of 999 to ensure a replicable behaviour for your Machine Learning algorithm."
      ],
      "metadata": {
        "id": "_hkbTV5k-I2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(999)\n",
        "\n",
        "cleanup_data = [(cleanup_review(r), l) for r,l in nltk_data]\n",
        "\n",
        "np.random.shuffle(cleanup_data)\n",
        "\n",
        "train_nltk_data = cleanup_data[0:int(len(cleanup_data)*0.85)]\n",
        "test_nltk_data = cleanup_data[int(len(cleanup_data)*0.85):int(len(cleanup_data)*0.95)]\n",
        "valid_nltk_data = cleanup_data[int(len(cleanup_data)*0.95):]"
      ],
      "metadata": {
        "id": "P2fd4WpQvjU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part 3 - Machine Learning model\n",
        "\n",
        "In this last section, we will implement a statistical machine learning model to fit the dataset and correctly predict the sentiment on new reviews. We will use scikit-learn for this section, and experiment with different classes of ML models.\n",
        "\n",
        "Scikit-learn offers a data structure called the `Pipeline` which allows to combine data transformation functions and machine learning algorithms into a single object. We will rely on that for this section, and combine a Support Vector Classifier with a TF-IDF Vectorizer. The former is a type of machine learning classification algorithm that is known to work particularly well with text classification tasks such as Sentiment Anaylsis, while the latter is a type of vectorizer that converts each word in a document to a numerical ID and weighs it according to how often it appears in the review and in the rest of the corpus; the algorithm prioritizes words that are popular across all reviews, and gives less priority to words that are specific to a single review but will not help the model to generalize to unseen ones."
      ],
      "metadata": {
        "id": "lf9Da8Ife5uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 3.a** Create a scikit-learn pipeline comprised of two separate components:\n",
        "- A TfidfVectorizer with min_df=3 and max_df=0.95 called \"vect\"\n",
        "- A LinearSVC classifier with C=1000 called \"clf\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mTdKsqxjfWxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "  ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
        "  ('clf', LinearSVC(C=1000, max_iter=10000)),\n",
        "])"
      ],
      "metadata": {
        "id": "lBJ8bmS2cLfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 3.b** Train your pipeline with the `fit` method, giving it a list of training samples and a list of labels associated with those samples. Use your `grammar_training_dataset` for this step\n",
        "\n"
      ],
      "metadata": {
        "id": "G_hs-fAstDQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.fit([d[0] for d in grammar_training_dataset], [d[1] for d in grammar_training_dataset])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "nByOGe7BtAeK",
        "outputId": "7f5ed9a0-6c8f-46f4-bc57-ed0dfcdc016a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vect', TfidfVectorizer(max_df=0.95, min_df=3)),\n",
              "                ('clf', LinearSVC(C=1000, max_iter=10000))])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(max_df=0.95, min_df=3)),\n",
              "                (&#x27;clf&#x27;, LinearSVC(C=1000, max_iter=10000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(max_df=0.95, min_df=3)),\n",
              "                (&#x27;clf&#x27;, LinearSVC(C=1000, max_iter=10000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.95, min_df=3)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=1000, max_iter=10000)</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 3.c** Run your pipeline on the `test_nltk_data`. Then, evaluate your pipeline by using the `classification_report` function from `sklearn.metrics`. Your performances will likely not be very good, due to the repetitive nature of data generated via grammars."
      ],
      "metadata": {
        "id": "pe4CNqzNVNQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "y_predicted = pipeline.predict([t[0] for t in test_nltk_data])\n",
        "\n",
        "# Print the classification report\n",
        "print(metrics.classification_report([t[1] for t in test_nltk_data], y_predicted,\n",
        "                                    target_names=['positive', 'negative']))\n",
        "\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.matshow(cm)\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z-UAPgws_0I",
        "outputId": "4326be8f-3d17-41e7-a398-6b07c042303b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.69      0.20      0.31       111\n",
            "    negative       0.47      0.89      0.61        89\n",
            "\n",
            "    accuracy                           0.51       200\n",
            "   macro avg       0.58      0.54      0.46       200\n",
            "weighted avg       0.59      0.51      0.44       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 3.d** Train your pipeline again, this time using your `train_nltk_data` dataset."
      ],
      "metadata": {
        "id": "j-b3KtfmVh6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "  ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
        "  ('clf', LinearSVC(C=1000, max_iter=10000)),\n",
        "])\n",
        "\n",
        "pipeline.fit([d[0] for d in train_nltk_data], [d[1] for d in train_nltk_data])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "c3uqQTpcs_NM",
        "outputId": "b43b4aea-5639-4a49-de85-5d8eaa73d64e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vect', TfidfVectorizer(max_df=0.95, min_df=3)),\n",
              "                ('clf', LinearSVC(C=1000, max_iter=10000))])"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(max_df=0.95, min_df=3)),\n",
              "                (&#x27;clf&#x27;, LinearSVC(C=1000, max_iter=10000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(max_df=0.95, min_df=3)),\n",
              "                (&#x27;clf&#x27;, LinearSVC(C=1000, max_iter=10000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.95, min_df=3)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=1000, max_iter=10000)</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 3.e** Evaluate the performances of your model using `classification_report`. The new model should be a lot more accurate than the previous one.\n",
        "\n"
      ],
      "metadata": {
        "id": "F7M6YJi7V1WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "y_predicted = pipeline.predict([t[0] for t in test_nltk_data])\n",
        "\n",
        "# Print the classification report\n",
        "print(metrics.classification_report([t[1] for t in test_nltk_data], y_predicted,\n",
        "                                    target_names=['positive', 'negative']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZVv4LbruYA8",
        "outputId": "3944f083-419b-42f0-b2e3-a1d29017f87d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.88      0.86      0.87       111\n",
            "    negative       0.83      0.85      0.84        89\n",
            "\n",
            "    accuracy                           0.85       200\n",
            "   macro avg       0.85      0.85      0.85       200\n",
            "weighted avg       0.86      0.85      0.86       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 3.f** Play with your code to familiarise with scikit-learn's suite of classifiers and parameters. Some experiments you may want to run include:\n",
        "- Training a model combining the `grammar_training_dataset` and `train_nltk_data` and verify whether it performs better than the `train_data` only model\n",
        "- Experiment with different classifiers -- for example, you may want to try a simple `GaussianNB` classifier, or try some classifiers that usually perform well on this task such as `AdaBoostClassifier` or `RandomForestClassifier`\n",
        "- Try changing the parameters of your classifiers -- for example, try reducing the C regularization parameter in your SVC, or increasing it further.  "
      ],
      "metadata": {
        "id": "kTnaNfhpWCm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.f (i) grammar_training_dataset and nltk_data combined\n",
        "\n",
        "pipeline=Pipeline([\n",
        "    ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
        "    ('clf', LinearSVC(C=1000, max_iter=10000)),\n",
        "    ])\n",
        "pipeline.fit([d[0] for d in train_nltk_data+grammar_training_dataset], [d[1] for d in train_nltk_data+grammar_training_dataset])\n",
        "y_predicted = pipeline.predict([t[0] for t in test_nltk_data])\n",
        "print(metrics.classification_report([t[1] for t in test_nltk_data], y_predicted,\n",
        "                                  target_names=['positive', 'negative']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4T85VIZW37Y",
        "outputId": "6b4598bc-e4ff-4a5b-dc04-ffd59cc96791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.89      0.83      0.86       111\n",
            "    negative       0.80      0.88      0.84        89\n",
            "\n",
            "    accuracy                           0.85       200\n",
            "   macro avg       0.85      0.85      0.85       200\n",
            "weighted avg       0.85      0.85      0.85       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.f (ii) classifiers experiments\n",
        "\n",
        "from sklearn.datasets import make_circles, make_classification, make_moons\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    SVC(gamma=2, C=1),\n",
        "    DecisionTreeClassifier(max_depth=5),\n",
        "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "    MLPClassifier(alpha=1, max_iter=1000),\n",
        "    AdaBoostClassifier()\n",
        "]\n",
        "for c in classifiers:\n",
        "  print(f\"Training classifier model: {c.__str__()}\")\n",
        "  pipeline=Pipeline([\n",
        "      ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
        "      ('clf', c),\n",
        "      ])\n",
        "  pipeline.fit(np.array([d[0] for d in train_nltk_data]), np.array([d[1] for d in train_nltk_data]))\n",
        "  y_predicted = pipeline.predict([t[0] for t in valid_nltk_data])\n",
        "  print(metrics.classification_report([t[1] for t in valid_nltk_data], y_predicted,\n",
        "                                    target_names=['positive', 'negative']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5k7Y7fmvsca",
        "outputId": "719ebb72-63ba-4f84-b41b-f0a0ceb193b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classifier model: KNeighborsClassifier(n_neighbors=3)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.79      0.69      0.74        55\n",
            "    negative       0.67      0.78      0.72        45\n",
            "\n",
            "    accuracy                           0.73       100\n",
            "   macro avg       0.73      0.73      0.73       100\n",
            "weighted avg       0.74      0.73      0.73       100\n",
            "\n",
            "Training classifier model: SVC(C=1, gamma=2)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.89      0.87      0.88        55\n",
            "    negative       0.85      0.87      0.86        45\n",
            "\n",
            "    accuracy                           0.87       100\n",
            "   macro avg       0.87      0.87      0.87       100\n",
            "weighted avg       0.87      0.87      0.87       100\n",
            "\n",
            "Training classifier model: DecisionTreeClassifier(max_depth=5)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.67      0.53      0.59        55\n",
            "    negative       0.54      0.69      0.61        45\n",
            "\n",
            "    accuracy                           0.60       100\n",
            "   macro avg       0.61      0.61      0.60       100\n",
            "weighted avg       0.62      0.60      0.60       100\n",
            "\n",
            "Training classifier model: RandomForestClassifier(max_depth=5, max_features=1, n_estimators=10)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.62      0.09      0.16        55\n",
            "    negative       0.46      0.93      0.61        45\n",
            "\n",
            "    accuracy                           0.47       100\n",
            "   macro avg       0.54      0.51      0.39       100\n",
            "weighted avg       0.55      0.47      0.36       100\n",
            "\n",
            "Training classifier model: MLPClassifier(alpha=1, max_iter=1000)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.94      0.82      0.87        55\n",
            "    negative       0.81      0.93      0.87        45\n",
            "\n",
            "    accuracy                           0.87       100\n",
            "   macro avg       0.87      0.88      0.87       100\n",
            "weighted avg       0.88      0.87      0.87       100\n",
            "\n",
            "Training classifier model: AdaBoostClassifier()\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.86      0.78      0.82        55\n",
            "    negative       0.76      0.84      0.80        45\n",
            "\n",
            "    accuracy                           0.81       100\n",
            "   macro avg       0.81      0.81      0.81       100\n",
            "weighted avg       0.81      0.81      0.81       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5TLUf5ppwNg2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}